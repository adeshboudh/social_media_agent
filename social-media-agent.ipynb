{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SAMAR\\anaconda3\\envs\\agent\\Lib\\site-packages\\pydantic\\_internal\\_config.py:295: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  warnings.warn(DEPRECATION_MESSAGE, DeprecationWarning)\n",
      "c:\\Users\\SAMAR\\anaconda3\\envs\\agent\\Lib\\site-packages\\pydantic\\_internal\\_fields.py:192: UserWarning: Field name \"schema\" in \"DatabricksQueryToolSchema\" shadows an attribute in parent \"BaseModel\"\n",
      "  warnings.warn(\n",
      "c:\\Users\\SAMAR\\anaconda3\\envs\\agent\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py:502: UserWarning: <built-in function callable> is not a Python type (it may be an instance of an object), Pydantic will allow any object with no validation since we cannot even enforce that the input is an instance of the given type. To get rid of this error wrap the type with `pydantic.SkipValidation`.\n",
      "  warn(\n",
      "c:\\Users\\SAMAR\\anaconda3\\envs\\agent\\Lib\\site-packages\\crewai_tools\\tools\\scrapegraph_scrape_tool\\scrapegraph_scrape_tool.py:34: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  @validator(\"website_url\")\n",
      "c:\\Users\\SAMAR\\anaconda3\\envs\\agent\\Lib\\site-packages\\crewai_tools\\tools\\selenium_scraping_tool\\selenium_scraping_tool.py:26: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  @validator(\"website_url\")\n",
      "c:\\Users\\SAMAR\\anaconda3\\envs\\agent\\Lib\\site-packages\\crewai_tools\\tools\\vision_tool\\vision_tool.py:15: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  @validator(\"image_path_url\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from crewai import LLM\n",
    "from typing import Type\n",
    "from dotenv import load_dotenv\n",
    "from crewai_tools import SerperDevTool, FirecrawlCrawlWebsiteTool, FirecrawlScrapeWebsiteTool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "os.environ[\"GEMINI_API_KEY\"] = os.getenv(\"GEMINI_API_KEY\")\n",
    "os.environ[\"SERPER_API_KEY\"] = os.getenv(\"SERPER_API_KEY\")\n",
    "os.environ[\"FIRECRAWL_API_KEY\"] = os.getenv(\"FIRECRAWL_API_KEY\")\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "os.environ[\"EXA_API_KEY\"] = os.getenv(\"EXA_API_KEY\")\n",
    "os.environ[\"OPENROUTER_API_KEY\"] = os.getenv(\"OPENROUTER_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the search tool\n",
    "search_tool = SerperDevTool()\n",
    "\n",
    "# Run a sample search query\n",
    "results = search_tool.run(search_query=\"Kerala Tourusim\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_tool = FirecrawlScrapeWebsiteTool(api_key=\"fc-599cd87f720148d29800f05c3be01424\")\n",
    "\n",
    "scrape_result = scrape_tool.run(url=\"https://www.keralatourism.org/\")\n",
    "print(scrape_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_data = {\n",
    "  \"clarity\": {\n",
    "    \"score\": 8.5,\n",
    "    \"explanation\": \"The message is clear and easy to understand, but could benefit from more concise wording.\"\n",
    "  },\n",
    "  \"tone\": {\n",
    "    \"score\": 7.0,\n",
    "    \"explanation\": \"The tone aligns with the brand's voice but lacks the emotional or persuasive depth needed to fully resonate with the audience.\"\n",
    "  },\n",
    "  \"engagement\": {\n",
    "    \"score\": 6.5,\n",
    "    \"explanation\": \"The post includes a question to prompt interaction, but the call to action is weak and could be more compelling.\"\n",
    "  },\n",
    "  \"platform_alignment\": {\n",
    "    \"score\": 9.0,\n",
    "    \"explanation\": \"The post is well-suited for Instagram, with visual appeal and a conversational style that fits the platform's culture.\"\n",
    "  },\n",
    "  \"hashtags\": {\n",
    "    \"score\": 5.5,\n",
    "    \"explanation\": \"Some hashtags are relevant, but others are overly generic or lack strategic alignment with the post's goals.\"\n",
    "  },\n",
    "  \"overall_effectiveness\": {\n",
    "    \"score\": 7.3,\n",
    "    \"explanation\": \"Overall, the post is effective but has room for improvement. To improve, consider refining the tone to be more engaging, strengthening the call to action, and optimizing hashtag selection for better reach and relevance.\"\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Overall, the post is effective but has room for improvement. To improve, consider refining the tone to be more engaging, strengthening the call to action, and optimizing hashtag selection for better reach and relevance.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_data['overall_effectiveness']['explanation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# loader = WebBaseLoader(\"https://www.keralatourism.org/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_list = [\"https://www.keralatourism.org/\", \n",
    "            \"http://www.keralatourism.gov.in/\", \n",
    "            \"https://www.makemytrip.com/holidays-india/kerala-tourism.html\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(url_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n\\n\\nExample Domain\\n\\n\\n\\n\\n\\n\\n\\nExample Domain\\nThis domain is for use in illustrative examples in documents. You may use this\\n    domain in literature without prior coordination or asking for permission.\\nMore information...\\n\\n\\n\\n',\n",
       " 'GoogleSearch Images Maps Play YouTube News Gmail Drive More »Web History | Settings | Sign in\\xa0Advanced searchGoogle offered in:  हिन्दी বাংলা తెలుగు मराठी தமிழ் ગુજરાતી ಕನ್ನಡ മലയാളം ਪੰਜਾਬੀ AdvertisingBusiness SolutionsAbout GoogleGoogle.co.in© 2025 - Privacy - Terms   ']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_content(urls):\n",
    "    loader = WebBaseLoader(urls)\n",
    "    docs = loader.load()\n",
    "    return [doc.page_content for doc in docs]\n",
    "\n",
    "\n",
    "get_content(url_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SAMAR\\anaconda3\\envs\\agent\\Lib\\typing.py:1570: RuntimeWarning: coroutine 'WebBaseLoader.fetch_all' was never awaited\n",
      "  return ((p, type(p)) for p in parameters)\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "from typing import Type, List\n",
    "from crewai.tools import BaseTool\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "\n",
    "class GetContentInput(BaseModel):\n",
    "    \"\"\"Input schema for GetContentTool.\"\"\"\n",
    "    urls: List[str] = Field(..., description=\"List of URLs to fetch content from.\")\n",
    "\n",
    "class GetContentTool(BaseTool):\n",
    "    name: str = \"get_content\"\n",
    "    description: str = \"Fetches and returns the content from the provided list of URLs.\"\n",
    "    args_schema: Type[BaseModel] = GetContentInput\n",
    "\n",
    "    def _run(self, urls: List[str]) -> List[str]:\n",
    "        \"\"\"Fetch content from the provided URLs.\"\"\"\n",
    "        loader = WebBaseLoader(urls)\n",
    "        docs = loader.load()\n",
    "        return [doc.page_content for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = loader.load()[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_multiple_pages = WebBaseLoader(\n",
    "    [\"https://www.example.com/\", \"https://google.com\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_multiple_pages.load()[1].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "for doc in loader.lazy_load():\n",
    "    docs.append(doc)\n",
    "print(docs[0].page_content)\n",
    "print(docs[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def web_base_loader(url: str) -> str:\n",
    "    loader = WebBaseLoader(url)\n",
    "    content = loader.load()[0].page_content\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_base_loader(\"https://www.keralatourism.org/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawl_tool = FirecrawlCrawlWebsiteTool(api_key=\"fc-599cd87f720148d29800f05c3be01424\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawl_tool._run(url=\"https://www.keralatourism.org/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool.run(url='firecrawl.dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from firecrawl import FirecrawlApp\n",
    "\n",
    "class CustomFirecrawlTool:\n",
    "    api_key=\"fc-1fa1181db69b4d2ea0c586b88ac34321\"\n",
    "    url=\"https://www.keralatourism.org/\"\n",
    "    def __init__(self, api_key):\n",
    "        self.firecrawl_app = FirecrawlApp(api_key=api_key)\n",
    "\n",
    "    def run(self, url):\n",
    "        # Define your parameters here\n",
    "        params = {\n",
    "            # Add necessary parameters based on the latest API documentation\n",
    "        }\n",
    "        return self.firecrawl_app.crawl_url(url, params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from firecrawl import FirecrawlApp\n",
    "\n",
    "app = FirecrawlApp(api_key='fc-1fa1181db69b4d2ea0c586b88ac34321')\n",
    "\n",
    "# Crawl a website:\n",
    "crawl_status = app.crawl_url(\n",
    "  'https://firecrawl.dev', \n",
    "  params={\n",
    "    'limit': 100, \n",
    "    'scrapeOptions': {'formats': ['markdown', 'html']}\n",
    "  },\n",
    "  poll_interval=30\n",
    ")\n",
    "print(crawl_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai_tools import FirecrawlCrawlWebsiteTool\n",
    "\n",
    "tool = FirecrawlCrawlWebsiteTool(url='firecrawl.dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install with pip install firecrawl-py\n",
    "from firecrawl import FirecrawlApp\n",
    "\n",
    "app = FirecrawlApp(api_key='fc-1fa1181db69b4d2ea0c586b88ac34321')\n",
    "\n",
    "crawl_result = app.crawl_url('https://docs.mendable.ai', params={\n",
    "'limit': 3,\n",
    "'scrapeOptions': {\n",
    "\t'formats': [ 'markdown' ],\n",
    "  }\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawl_tool = FirecrawlCrawlWebsiteTool(api_key=\"fc-1fa1181db69b4d2ea0c586b88ac34321\", max_depth=1)\n",
    "\n",
    "crawl_result = crawl_tool.run(url=\"https://www.keralatourism.org/\")\n",
    "options = {\n",
    "    \"crawlerOptions\": crawler_options,\n",
    "    \"timeout\": timeout,\n",
    "}\n",
    "return self._firecrawl.crawl_url(url, options)\n",
    "\n",
    "print(crawl_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from firecrawl import FirecrawlApp\n",
    "\n",
    "fc = FirecrawlApp(api_key=\"fc-1fa1181db69b4d2ea0c586b88ac34321\")\n",
    "res = fc.crawl_url(url=\"https://www.keralatourism.org/\")\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawl_tool = FirecrawlCrawlWebsiteTool()\n",
    "crawl_result = crawl_tool.run(url=\"https://www.keralatourism.org/\")\n",
    "print(crawl_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install with pip install firecrawl-py\n",
    "from firecrawl import FirecrawlApp\n",
    "\n",
    "app = FirecrawlApp(api_key=\"fc-1fa1181db69b4d2ea0c586b88ac34321\")\n",
    "\n",
    "crawl_result = app.crawl_url('https://www.keralatourism.org/', params={\n",
    "'limit': 5,\n",
    "'scrapeOptions': {\n",
    "\t'formats': [ 'markdown' ],\n",
    "  }\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawl_result = crawl_tool.run(\n",
    "    url=\"https://www.keralatourism.org/\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Type\n",
    "from crewai.tools import BaseTool\n",
    "from pydantic import BaseModel, Field\n",
    "from firecrawl import FirecrawlApp\n",
    "\n",
    "class FirecrawlToolInput(BaseModel):\n",
    "    \"\"\"Input schema for FirecrawlTool.\"\"\"\n",
    "    website_url: str = Field(\n",
    "        ..., \n",
    "        description=\"The URL of the website to scrape content from.\"\n",
    "    )\n",
    "\n",
    "class FirecrawlTool(BaseTool):\n",
    "    name: str = \"Firecrawl Web Scraper\"\n",
    "    description: str = (\n",
    "        \"A tool that scrapes content from websites and returns it in a format \"\n",
    "        \"suitable for LLM processing. Useful for gathering information from \"\n",
    "        \"specific web pages for research and content creation.\"\n",
    "    )\n",
    "    args_schema: Type[BaseModel] = FirecrawlToolInput\n",
    "\n",
    "    def _run(self, website_url: str) -> str:\n",
    "        try:\n",
    "            firecrawl_instance = FirecrawlApp(api_key=os.getenv(\"FIRECRAWL_API_KEY\"))\n",
    "            result = firecrawl_instance.scrape_url(website_url)\n",
    "            return result['markdown']\n",
    "        except Exception as e:\n",
    "            return f\"Error scraping website: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serper_tool = SerperDevTool()\n",
    "\n",
    "firecrawl_tool = FirecrawlTool()\n",
    "\n",
    "# llm_creator = LLM(model=\"gemini/gemini-1.5-flash\",\n",
    "#                    verbose=True, \n",
    "#                    temperature=0.5,\n",
    "#                    api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "\n",
    "# llm_manager = LLM(model=\"gemini/gemini-1.5-flash\",\n",
    "#                    verbose=True, \n",
    "#                    temperature=0.5,\n",
    "#                    api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "\n",
    "llm_creator = LLM(model=\"ollama/llama3.2:3b\",\n",
    "                   verbose=True, \n",
    "                   temperature=0.5,\n",
    "                   base_url=\"http://localhost:11434\")\n",
    "\n",
    "llm_manager = LLM(model=\"ollama/deepseek-r1:8b\",\n",
    "                  verbose=True,\n",
    "                  temperature=0.3,\n",
    "                  base_url=\"http://localhost:11434\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai_tools import FirecrawlCrawlWebsiteTool\n",
    "\n",
    "tool = FirecrawlCrawlWebsiteTool(\n",
    "    max_depth=2, \n",
    "    api_key=\"fc-e5b59793e6e8489eaa87d93c03aa98e4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"FIRECRAWL_API_KEY\"] = \"fc-e5b59793e6e8489eaa87d93c03aa98e4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from firecrawl.firecrawl import FirecrawlApp\n",
    "\n",
    "class CustomFirecrawlTool:\n",
    "    def __init__(self, api_key):\n",
    "        self.firecrawl = FirecrawlApp(api_key=api_key)\n",
    "\n",
    "    def crawl(self, url):\n",
    "        response = self.firecrawl.crawl_url(url)\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawl = tool.run('https://www.keralatourism.org/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawl = firecrawl_tool.run(\"https://www.keralatourism.org/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(crawl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serper_search_agent = Agent(\n",
    "    role=\"Google Search\",\n",
    "    goal=\"Perform a Google search for '{topic}' and extract all URL links from the JSON response.\",\n",
    "    backstory=\"\"\"\n",
    "    You are a search expert that retrieves structured search results. Your main responsibility is to extract URLs from the JSON response and store them into list.\n",
    "    \"\"\",\n",
    "    memory=True,\n",
    "    llm=llm_creator,\n",
    "    tools=[serper_tool],\n",
    "    allow_delegation=True,\n",
    ")\n",
    "\n",
    "serper_search_task = Task(\n",
    "    description=\"\"\"\n",
    "    Perform a Google search for '{topic}' using the Serper tool. Extract all URL links from the JSON response \"\n",
    "    and store them in a list.\n",
    "    \"\"\",\n",
    "    expected_output=\"A list of Top 5 URLs extracted from the JSON response.\",\n",
    "    agent=serper_search_agent,\n",
    "    tools=[serper_tool],\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "research_agent = Agent(\n",
    "    role=\"Content Researcher\",\n",
    "    goal=\"Process each URL to extract and format content, and select the top 2 to 4 image URLs (If Available).\",\n",
    "    backstory=\"You are skilled in extracting, summarizing, and organizing content and images from web pages.\",\n",
    "    verbose=True,\n",
    "    memory=True,\n",
    "    llm=llm_creator,\n",
    "    allow_delegation=False,\n",
    ")\n",
    "\n",
    "research_task = Task(\n",
    "    description=\"\"\"\n",
    "    Process each URL in the list using the `firecrawl_tool` to extract content. \n",
    "    For each URL, select the top 2 to 4 image URLs if available. \n",
    "    Merge all extracted content and provide a concise summary.\n",
    "    \"\"\",\n",
    "    expected_output=\"\"\"\n",
    "    A JSON object containing:\n",
    "     - 'platform': The platform for which the content is being prepared.\n",
    "     - 'content': Formatted and summarized text content.\n",
    "     - 'images': An array of the top 2 to 4 image URLs (if available in the crawled data).\n",
    "    \"\"\",\n",
    "    agent=research_agent,\n",
    "    tools=[firecrawl_tool],\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "research_manager = Agent(\n",
    "    role=\"Social Media Research Manager\",\n",
    "    goal=\"\"\"\n",
    "    Oversee and coordinate the research and content creation workflow for social media posts.\n",
    "\n",
    "    Responsibilities:\n",
    "    - Delegate tasks to specialized agents:\n",
    "      - **Serper Search Agent**: Conducts Google searches and extracts URLs.\n",
    "      - **Research Agent**: Processes URLs to gather and format content, selecting the top 2 to 4 image URLs(If Available).\n",
    "\n",
    "    - Ensure efficient task execution and smooth workflow management.\n",
    "    \"\"\",\n",
    "    backstory=\"\"\"\n",
    "    As an AI-powered project manager, you excel in coordinating research, content creation, and image selection tasks.\n",
    "    Your primary role is to assign tasks effectively and ensure the workflow progresses smoothly.\n",
    "    \"\"\",\n",
    "    verbose=True,\n",
    "    memory=True,\n",
    "    llm=llm_creator,\n",
    "    allow_delegation=True,\n",
    ")\n",
    "\n",
    "research_crew = Crew(\n",
    "    agents=[serper_search_agent, research_agent],\n",
    "    tasks=[serper_search_task, research_task],\n",
    "    verbose=True,\n",
    "    process=Process.hierarchical,\n",
    "    manager_agent=research_manager,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = research_crew.kickoff(inputs={'topic': 'Kerala Tourism', 'platform': 'Instagram'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facebook_agent = Agent(\n",
    "    role=\"Facebook Content Creator\",\n",
    "    goal=\"Generate an engaging Facebook post using the provided content and images, with relevant hashtags.\",\n",
    "    backstory=\"\"\"\n",
    "    You specialize in crafting highly shareable Facebook content. \n",
    "    Use the provided content and images to create a post optimized for engagement, including a set of relevant hashtags.\n",
    "    \"\"\",\n",
    "    verbose=True,\n",
    "    memory=True,\n",
    "    llm=llm_creator,\n",
    "    allow_delegation=False,\n",
    ")\n",
    "\n",
    "instagram_agent = Agent(\n",
    "    role=\"Instagram Content Creator\",\n",
    "    goal=\"Create a visually appealing Instagram post using the provided content and images, with engaging hashtags.\",\n",
    "    backstory=\"\"\"\n",
    "    You are a creative Instagram expert, skilled in curating visually appealing posts. \n",
    "    Use the provided content and images to create an engaging Instagram post with well-researched hashtags.\n",
    "    \"\"\",\n",
    "    verbose=True,\n",
    "    llm=llm_creator,\n",
    "    allow_delegation=False,\n",
    ")\n",
    "\n",
    "twitter_agent = Agent(\n",
    "    role=\"Twitter Content Creator\",\n",
    "    goal=\"Craft a concise and engaging tweet using the provided content and images, with strategic hashtags.\",\n",
    "    backstory=\"You are an expert in creating highly shareable tweets. Use the provided content and images to craft an engaging tweet with strategic hashtags.\",\n",
    "    verbose=True,\n",
    "    llm=llm_creator,\n",
    "    allow_delegation=False,\n",
    ")\n",
    "\n",
    "linkedin_agent = Agent(\n",
    "    role=\"LinkedIn Content Creator\",\n",
    "    goal=\"Generate a professional LinkedIn post using the provided content and images, with industry-relevant hashtags.\",\n",
    "    backstory=\"You are a LinkedIn business content expert. Use the provided content and images to create a professional LinkedIn post with industry-relevant hashtags.\",\n",
    "    verbose=True,\n",
    "    llm=llm_creator,\n",
    "    allow_delegation=False,\n",
    ")\n",
    "\n",
    "facebook_task = Task(\n",
    "    description=\"Create a Facebook post using the provided content and images, including relevant hashtags.\",\n",
    "    expected_output=\"A high-quality Facebook post with text, a set of relevant hashtags and a list of image url's (is available).\",\n",
    "    agent=facebook_agent,\n",
    ")\n",
    "\n",
    "instagram_task = Task(\n",
    "    description=\"Generate an Instagram post using the provided content and images, with a compelling caption and hashtags.\",\n",
    "    expected_output=\"A well-crafted Instagram post with a caption, a set of well-researched hashtags and list of image url's (if available).\",\n",
    "    agent=instagram_agent,\n",
    ")\n",
    "\n",
    "twitter_task = Task(\n",
    "    description=\"Generate a Twitter post using the provided content and images, including strategic hashtags.\",\n",
    "    expected_output=\"A high-impact tweet with a message, and strategic hashtags and list of image url's (if available).\",\n",
    "    agent=twitter_agent,\n",
    ")\n",
    "\n",
    "linkedin_task = Task(\n",
    "    description=\"Generate a LinkedIn post using the provided content and images, with professional hashtags.\",\n",
    "    expected_output=\"A professional LinkedIn post with content, industry-relevant hashtags and list of image url's (if available).\",\n",
    "    agent=linkedin_agent,\n",
    ")\n",
    "\n",
    "\n",
    "post_manager = Agent(\n",
    "    role=\"Social Media Project Manager\",\n",
    "    goal=\"\"\"\n",
    "    Efficiently manage the creation of a social media post for the specified platform using content and images from the research crew.\n",
    "\n",
    "    - Assign tasks to the appropriate content creator agent based on the platform:\n",
    "      - **Facebook Content Creator** for Facebook posts.\n",
    "      - **Instagram Content Creator** for Instagram posts.\n",
    "      - **Twitter Content Creator** for Twitter posts.\n",
    "      - **LinkedIn Content Creator** for LinkedIn posts.\n",
    "\n",
    "    - Use the JSON input from the research crew, which includes 'content' and 'images', to guide the post creation.\n",
    "    - Ensure the post is optimized for engagement and aligns with platform-specific best practices.\n",
    "    \"\"\",\n",
    "    backstory=\"\"\"\n",
    "    You are an experienced AI-powered manager, skilled in overseeing content creation for social media. \n",
    "    Your role is to efficiently assign tasks to specialized agents and ensure the workflow progresses smoothly.\n",
    "    \"\"\",\n",
    "    verbose=True,\n",
    "    memory=True,\n",
    "    llm=llm_creator,\n",
    "    allow_delegation=True,\n",
    ")\n",
    "\n",
    "post_crew = Crew(\n",
    "    agents=[facebook_agent, instagram_agent, twitter_agent, linkedin_agent],\n",
    "    tasks=[facebook_task, instagram_task, twitter_task, linkedin_task],\n",
    "    process=Process.hierarchical,\n",
    "    verbose=True,\n",
    "    manager_agent=post_manager\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from crewai.flow import Flow, listen, start\n",
    "from typing import Dict, List\n",
    "import json\n",
    "\n",
    "class PostState(BaseModel):\n",
    "    content: Dict[str, str] = {}\n",
    "    images: List[str] = []\n",
    "    platform: str = \"\"\n",
    "    post: str = \"\"\n",
    "\n",
    "class PostFlow(Flow[PostState]):\n",
    "\n",
    "    @start()\n",
    "    def generate_content(self):\n",
    "        print(\"Generating content\")\n",
    "        result = (\n",
    "            research_crew.kickoff(inputs={'topic': 'Kerala Tourism', 'platform': 'Instagram'})\n",
    "        )\n",
    "\n",
    "        print(\"Post generated\", result.raw)\n",
    "        self.state.content = result.raw\n",
    "\n",
    "    @listen(generate_content)\n",
    "    def generate_post(self, content):\n",
    "        print(\"Generating post\")\n",
    "           \n",
    "        result = post_crew.kickoff(content)\n",
    "        \n",
    "        print(\"Post generated\", result.raw)\n",
    "        print(f\"Post: {result.raw}\")\n",
    "        self.state.post = result.raw\n",
    "\n",
    "    @listen(generate_post)\n",
    "    def save_post(self, post):\n",
    "        print(\"Saving Post\")\n",
    "        if post is None:\n",
    "            print(\"Post is None, skipping write operation.\")\n",
    "            return  # Exit the function early if post is None\n",
    "        \n",
    "        with open(\"plan.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "import asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "def kickoff():\n",
    "    post_flow = PostFlow()\n",
    "    asyncio.run(post_flow.kickoff_async())  # Use kickoff_async directly\n",
    "\n",
    "def plot():\n",
    "    post_flow = PostFlow()\n",
    "    post_flow.plot()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    kickoff()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
